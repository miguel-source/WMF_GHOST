{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMHQs-tF_cHi"
   },
   "source": [
    "# Install packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##!pip install geopandas\n",
    "##! pip install earthengine-api --upgrade\n",
    "##!pip install rasterio\n",
    "##!pip install gdal\n",
    "#!pip install osgeo\n",
    "##!conda install -c conda-forge gdal\n",
    "##!pip install pysheds\n",
    "!pip install cartopy\n",
    "!pip install deap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import rasterio\n",
    "import cartopy\n",
    "import sys\n",
    "import pylab as pl\n",
    "from osgeo import gdal, osr\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.transform import from_origin\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.transform import from_origin\n",
    "from rasterio.mask import mask\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.transform import from_origin\n",
    "from rasterio.mask import mask\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "from rasterio.enums import Resampling\n",
    "import richdem as rd\n",
    "import whitebox\n",
    "import os\n",
    "\n",
    "##import gdal\n",
    "##!pip show gdal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install geopandas rasterio numpy\n",
    "#!pip install richdem\n",
    "#!pip install whitebox\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T15:34:25.379621Z",
     "start_time": "2023-05-16T15:34:24.009628Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from wmf import wmf \n",
    "import pylab as pl \n",
    "from osgeo import gdal\n",
    "from wmf import ghost_topo as go \n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKJ9dzcGAlxJ"
   },
   "source": [
    "# Set up a watershed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mcAAuhNSB9mQ"
   },
   "source": [
    "## Read and process DEM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def convert_map_to_tif(input_map, output_tif):\n",
    "    # Open the PCRaster .map file\n",
    "    ds = gdal.Open(input_map)\n",
    "    if ds is None:\n",
    "        raise Exception(\"Could not open input file\")\n",
    "\n",
    "    # Create the GeoTIFF driver\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "\n",
    "    # Create the GeoTIFF output file\n",
    "    ds_tif = driver.CreateCopy(output_tif, ds)\n",
    "\n",
    "    # Close the datasets\n",
    "    ds = None\n",
    "    ds_tif = None\n",
    "\n",
    "    print(\"Conversion complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_map_file = \"Dir30.map\"\n",
    "    output_tif_file = \"OT30.tif\"\n",
    "    convert_map_to_tif(input_map_file, output_tif_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T15:34:28.596764Z",
     "start_time": "2023-05-16T15:34:28.265059Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "executionInfo": {
     "elapsed": 388,
     "status": "error",
     "timestamp": 1684247125015,
     "user": {
      "displayName": "Nicolás Velásquez",
      "userId": "15649486147728645342"
     },
     "user_tz": 300
    },
    "id": "qiHo4qP7AWmL",
    "outputId": "6ff1bb5b-f07b-4c68-b3f8-efc23b3a854d"
   },
   "source": [
    "from wmf import wmf\n",
    "from osgeo import gdal\n",
    "DEM, epsg = wmf.read_map_raster('dem2.tif', isDEMorDIR=True, dxp=90, noDataP=-9999)\n",
    "DIR, epsg = wmf.read_map_raster('dir.tif', isDEMorDIR=True, dxp=90, noDataP=-9999, isDIR=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raster resampling and scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def multiply_raster_values(input_path, output_path, factor):\n",
    "    # Open the original raster\n",
    "    with rasterio.open(input_path) as src:\n",
    "        # Read the raster data and mask\n",
    "        data = src.read(1, masked=True)  # Read band 1 with masking\n",
    "\n",
    "\n",
    "        # Multiply the raster values by the factor\n",
    "        data = data * factor\n",
    "        \n",
    "        # Update metadata\n",
    "        meta = src.meta.copy()\n",
    "        \n",
    "        # Write the modified raster to a new file\n",
    "        with rasterio.open(output_path, 'w', **meta) as dst:\n",
    "            dst.write(data.filled(meta['nodata']), 1)  # Write to band 1, using the original NoData value\n",
    "\n",
    "def resample_raster(input_path, output_path, new_cell_size):\n",
    "    # Open the modified raster\n",
    "    with rasterio.open(input_path) as src:\n",
    "        # Calculate new dimensions based on new cell size\n",
    "        old_cell_size_x, old_cell_size_y = src.res\n",
    "        scale_factor_x = old_cell_size_x / new_cell_size\n",
    "        scale_factor_y = old_cell_size_y / new_cell_size\n",
    "        \n",
    "        new_width = int(src.width * scale_factor_x)\n",
    "        new_height = int(src.height * scale_factor_y)\n",
    "\n",
    "        # Define new transformation\n",
    "        transform = src.transform * src.transform.scale(\n",
    "            (src.width / new_width),\n",
    "            (src.height / new_height)\n",
    "        )\n",
    "\n",
    "        # Set the new metadata\n",
    "        new_meta = src.meta.copy()\n",
    "        new_meta.update({\n",
    "            'driver': 'GTiff',\n",
    "            'height': new_height,\n",
    "            'width': new_width,\n",
    "            'transform': transform\n",
    "        })\n",
    "\n",
    "        # Write the resampled raster\n",
    "        with rasterio.open(output_path, 'w', **new_meta) as dst:\n",
    "            dst.write(\n",
    "                src.read(\n",
    "                    1,\n",
    "                    out_shape=(new_height, new_width),\n",
    "                    resampling=Resampling.bilinear\n",
    "                ),\n",
    "                1  # Writing to band 1\n",
    "            )\n",
    "\n",
    "# File paths\n",
    "input_raster_path = \"/mnt/c/Users/Miguel/desktop/wmf/Upper IA MN - CT 16/NewDem.tif\"\n",
    "temp_raster_path = \"/mnt/c/Users/Miguel/desktop/wmf/Upper IA MN - CT 16/temp_dem.tif\"\n",
    "final_raster_path = \"/mnt/c/Users/Miguel/desktop/Upper IA MN - CT 16/dem.tif\"\n",
    "\n",
    "# Parameters\n",
    "factor = 0.01\n",
    "new_cell_size = 20  # Desired new cell size in units of the raster's coordinate system\n",
    "\n",
    "# Perform operations\n",
    "multiply_raster_values(input_raster_path, temp_raster_path, factor)\n",
    "resample_raster(temp_raster_path, final_raster_path, new_cell_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def calculate_flow_direction(dem_path, flow_dir_path):\n",
    "    wbt = whitebox.WhiteboxTools()\n",
    "    \n",
    "    # Ensure that the whitebox tools executable is in the system path\n",
    "    wbt.set_whitebox_dir(os.path.dirname(whitebox.__file__))\n",
    "    \n",
    "    # Calculate flow directions using D8 algorithm\n",
    "    wbt.d8_pointer(dem=dem_path, output=flow_dir_path)\n",
    "\n",
    "# File paths\n",
    "final_raster_path = \"/mnt/c/Users/Miguel/desktop/wmf/Boone_CT 16/dem.tif\"\n",
    "flow_dir_path = \"/mnt/c/Users/Miguel/desktop/wmf/Boone_CT 16/dir.tif\"\n",
    "\n",
    "# Calculate flow direction\n",
    "calculate_flow_direction(final_raster_path, flow_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value modifications completed. Modified file saved as 'dir1.tif'.\n"
     ]
    }
   ],
   "source": [
    "from wmf import wmf\n",
    "from osgeo import gdal\n",
    "DEM, epsg = wmf.read_map_raster('dem.tif', isDEMorDIR=True, dxp=30, noDataP=-9999)\n",
    "DIR, epsg = wmf.read_map_raster('dir.tif', isDEMorDIR=True, dxp=30, noDataP=-9999, isDIR=False)\n",
    "# Define the value modification dictionary\n",
    "value_mapping = {32: 7, 64: 8, 128: 9, 16: 4, 1: 6, 8: 1, 4: 2, 2: 3}\n",
    "\n",
    "# Apply the value modifications using NumPy\n",
    "modified_DIR = np.copy(DIR)\n",
    "for orig_value, new_value in value_mapping.items():\n",
    "    modified_DIR[DIR == orig_value] = new_value\n",
    "\n",
    "# Set null values to 0\n",
    "modified_DIR[modified_DIR == -9999] = 0\n",
    "\n",
    "# Open the original dataset to get geotransformation and spatial reference\n",
    "original_dataset = gdal.Open('dir.tif')\n",
    "geotransform = original_dataset.GetGeoTransform()\n",
    "spatial_ref = original_dataset.GetProjection()\n",
    "\n",
    "# Write the modified data (including null values) back to the raster file\n",
    "output_file = 'dir1.tif'\n",
    "driver = gdal.GetDriverByName('GTiff')\n",
    "output_dataset = driver.Create(output_file, DIR.shape[0], DIR.shape[1], 1, gdal.GDT_Byte)  # Transposed shape\n",
    "output_band = output_dataset.GetRasterBand(1)\n",
    "output_band.WriteArray(modified_DIR.T)  # Transpose the modified data array\n",
    "output_band.SetNoDataValue(0)  # Set null value to 0\n",
    "\n",
    "# Set the same spatial reference as the original dataset\n",
    "output_dataset.SetProjection(spatial_ref)\n",
    "\n",
    "# Set the same geotransformation as the original dataset\n",
    "output_dataset.SetGeoTransform(geotransform)\n",
    "\n",
    "output_band.FlushCache()\n",
    "output_band = None\n",
    "output_dataset = None\n",
    "original_dataset = None\n",
    "\n",
    "print(\"Value modifications completed. Modified file saved as 'dir1.tif'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DIR, epsg = wmf.read_map_raster('dir30c.tif', isDEMorDIR=True, dxp=90, noDataP=-9999, isDIR=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wmf import wmf\n",
    "from osgeo import gdal\n",
    "DEM, epsg = wmf.read_map_raster('dem.tif', isDEMorDIR=True, dxp=30, noDataP=-9999)\n",
    "DIR, epsg = wmf.read_map_raster('dir1.tif', isDEMorDIR=True, dxp=30, noDataP=-9999, isDIR=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6W4UBYpLCB5_"
   },
   "source": [
    "## Extract watershed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T15:34:29.125615Z",
     "start_time": "2023-05-16T15:34:29.040644Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 115,
     "status": "ok",
     "timestamp": 1629919310394,
     "user": {
      "displayName": "Nicolás Velásquez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj_p27Ww8CzJCwMccm7KvzOMrLqhxfXelzHuWR1GA=s64",
      "userId": "15649486147728645342"
     },
     "user_tz": 300
    },
    "id": "cmpVxRSNBlOI",
    "outputId": "7290e2d8-9046-4ed0-e002-0b7460863200",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "st = wmf.Stream(610113.447, 4823891.04, DEM, DIR) # Top point stream\n",
    "cu = wmf.SimuBasin(611989.447, 4811619.04, DEM, DIR, stream=st, threshold=50) # Catchment\n",
    "Cat3, prop, epsg = wmf.read_map_raster('dir1.tif')\n",
    "Cat2, prop, epsg = wmf.read_map_raster('dem.tif')\n",
    "# Cat, prop, epsg = wmf.read_map_raster('dem.tif')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cat3Basin = cu.Transform_Map2Basin(Cat3, prop)\n",
    "Cat2Basin = cu.Transform_Map2Basin(Cat2, prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "cu.Plot_basinClean(Cat3Basin), cu.Plot_basinClean(Cat2Basin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "cu.Save_Basin2Map('Basin.shp',\n",
    "    dx=30,\n",
    "    Param={},\n",
    "    DriverFormat='ESRI Shapefile',\n",
    "    EPSG=26915,\n",
    "    GeoParam=False,\n",
    ")\n",
    "cu.Save_Net2Map('Net.shp',\n",
    "    dx=30,\n",
    "    threshold=300,\n",
    "    qmed=None,\n",
    "    Dict=None,\n",
    "    DriverFormat='ESRI Shapefile',\n",
    "    EPSG=26915,\n",
    "    Numlink_id=True,\n",
    "    formato='%.2f',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Focus_Dict Raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the input shapefiles\n",
    "soapcreek_shapefile = \"/mnt/c/Users/Miguel/desktop/wmf/Upper IA MN - CT 16/Basin.shp\"\n",
    "net_buff_shapefile = \"/mnt/c/Users/Miguel/desktop/wmf/Upper IA MN - CT 16/NET_Buff.shp\"\n",
    "make_ingrid_shapefile = \"/mnt/c/Users/Miguel/desktop/wmf/Upper IA MN - CT 16/Pool.shp\"\n",
    "\n",
    "# Define the output raster\n",
    "output_raster = \"/mnt/c/Users/Miguel/desktop/wmf/Upper IA MN - CT 16/focus_dic.tif\"\n",
    "\n",
    "# Read the shapefiles using geopandas\n",
    "soapcreek_gdf = gpd.read_file(soapcreek_shapefile)\n",
    "net_buff_gdf = gpd.read_file(net_buff_shapefile)\n",
    "make_ingrid_gdf = gpd.read_file(make_ingrid_shapefile)\n",
    "\n",
    "# Ensure the GeoDataFrames are in the correct projected CRS (NAD_1983_UTM_Zone_15N)\n",
    "soapcreek_gdf = soapcreek_gdf.to_crs(epsg=26915)\n",
    "net_buff_gdf = net_buff_gdf.to_crs(epsg=26915)\n",
    "make_ingrid_gdf = make_ingrid_gdf.to_crs(epsg=26915)\n",
    "\n",
    "# Define raster properties\n",
    "cell_size = 20\n",
    "xmin, ymin, xmax, ymax = soapcreek_gdf.total_bounds\n",
    "width = int((xmax - xmin) / cell_size)\n",
    "height = int((ymax - ymin) / cell_size)\n",
    "transform = from_origin(xmin, ymax, cell_size, cell_size)\n",
    "\n",
    "# Function to rasterize a GeoDataFrame\n",
    "def rasterize_gdf(gdf, transform, out_shape, zone_value):\n",
    "    shapes = ((geom, zone_value) for geom in gdf.geometry)\n",
    "    return rasterize(shapes, transform=transform, out_shape=out_shape, fill=0, dtype=rasterio.uint8)\n",
    "\n",
    "# Create empty raster initialized with NoData (value 0)\n",
    "combined_raster = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "# Rasterize Make_Ingrid layer and assign zone value 3\n",
    "combined_raster = rasterize_gdf(make_ingrid_gdf, transform, (height, width), 3)\n",
    "\n",
    "# Rasterize SoapCreek layer and assign zone value 1, ensuring it doesn't overwrite existing values\n",
    "soapcreek_raster = rasterize_gdf(soapcreek_gdf, transform, (height, width), 1)\n",
    "combined_raster[combined_raster == 0] = soapcreek_raster[combined_raster == 0]\n",
    "\n",
    "# Rasterize NET_Buff layer and assign zone value 2, ensuring it overwrites existing values where applicable\n",
    "net_buff_raster = rasterize_gdf(net_buff_gdf, transform, (height, width), 2)\n",
    "combined_raster[net_buff_raster != 0] = 2\n",
    "\n",
    "# Debug: Plot the combined raster before masking\n",
    "plt.imshow(combined_raster, cmap='gray')\n",
    "plt.title('Combined Raster Before Masking')\n",
    "plt.show()\n",
    "\n",
    "# Create a temporary file to store the combined raster\n",
    "with tempfile.NamedTemporaryFile(suffix='.tif') as temp_raster:\n",
    "    # Write the combined raster to the temporary file\n",
    "    with rasterio.open(\n",
    "        temp_raster.name, 'w', driver='GTiff', height=height, width=width, count=1, dtype=rasterio.uint8, transform=transform, crs=soapcreek_gdf.crs.to_string()\n",
    "    ) as mem_raster:\n",
    "        mem_raster.write(combined_raster, 1)\n",
    "    \n",
    "    # Reopen the temporary file in read mode to apply the mask\n",
    "    with rasterio.open(temp_raster.name) as src:\n",
    "        soapcreek_geometry = [geom for geom in soapcreek_gdf.geometry]\n",
    "        out_image, out_transform = mask(src, soapcreek_geometry, invert=False)\n",
    "        out_image = out_image[0]  # Extract the array from the masked result\n",
    "\n",
    "# Set all 0 values to NoData\n",
    "out_image[out_image == 0] = 255  # Use 255 as the NoData value for an 8-bit raster\n",
    "\n",
    "# # Debug: Plot the masked raster\n",
    "# plt.imshow(out_image, cmap='Blues')\n",
    "# plt.title('Masked Raster')\n",
    "# plt.show()\n",
    "\n",
    "# Write the masked raster to a file\n",
    "with rasterio.open(\n",
    "    output_raster, 'w', driver='GTiff', height=out_image.shape[0], width=out_image.shape[1], count=1,\n",
    "    dtype=rasterio.uint8, crs=soapcreek_gdf.crs.to_string(), transform=out_transform, nodata=255\n",
    ") as dst:\n",
    "    dst.write(out_image, 1)\n",
    "\n",
    "print(f\"Raster with zones created at {output_raster}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cat, prop, epsg = wmf.read_map_raster('focus_dic.tif')\n",
    "CatBasin = cu.Transform_Map2Basin(Cat, prop)\n",
    "cu.Plot_basinClean(CatBasin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uzo34TNJCIme"
   },
   "source": [
    "## Get ghost class and its elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T15:34:34.663198Z",
     "start_time": "2023-05-16T15:34:34.384127Z"
    }
   },
   "outputs": [],
   "source": [
    "gp = go.ghost_preprocess(cu, path_dem='dem.tif', \n",
    "   seg_threshold=50,\n",
    "  seg_point_distance=4,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T15:35:34.494385Z",
     "start_time": "2023-05-16T15:35:34.297815Z"
    }
   },
   "outputs": [],
   "source": [
    "corrected = gp.get_segments_topology(epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T15:36:35.840495Z",
     "start_time": "2023-05-16T15:36:35.799615Z"
    }
   },
   "outputs": [],
   "source": [
    "gp.get_mesh_river_points(clean_close_points=True,\n",
    "                         min_river2river_distance=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T15:37:18.159584Z",
     "start_time": "2023-05-16T15:37:17.992723Z"
    }
   },
   "outputs": [],
   "source": [
    "gp.get_mesh_grid_points(mesh_spaces=10,\n",
    "                        border_iter=5,\n",
    "                        clean_with_river=True,\n",
    "                        min_dem2river_distance=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd.DataFrame(gp.mesh_points_river.T, columns = ['x','y']).to_csv('mesh_coordinates_river.csv')\n",
    "pd.DataFrame(gp.mesh_points_dem.T, columns = ['x','y']).to_csv('mesh_coordinates_grid.csv')\n",
    "pd.DataFrame(gp.mesh_points_boundary[0].T, columns = ['x','y'])kkz.to_csv('mesh_coordinates_boundary.csv')\n",
    "pd.DataFrame(gp.mesh_points_boundary[1].T, columns = ['x','y']).to_csv('mesh_coordinates_boundary2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T15:37:54.007199Z",
     "start_time": "2023-05-16T15:37:37.064336Z"
    }
   },
   "outputs": [],
   "source": [
    "gp.get_voronoi_polygons()\n",
    "gp.define_polygons_topology()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T15:41:52.084963Z",
     "start_time": "2023-05-16T15:41:46.005991Z"
    }
   },
   "outputs": [],
   "source": [
    "gp.write_mesh_file('mesh_4.mesh', shp_path='mesh_4.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T15:44:34.783358Z",
     "start_time": "2023-05-16T15:44:34.184008Z"
    }
   },
   "outputs": [],
   "source": [
    "gp.write_river_file('river_4.riv',shp_path='river_4.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T15:48:20.074543Z",
     "start_time": "2023-05-16T15:48:20.047862Z"
    }
   },
   "source": [
    "gp.write_attribute_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "executionInfo": {
     "elapsed": 10659,
     "status": "error",
     "timestamp": 1629753971021,
     "user": {
      "displayName": "Sam Landsteiner",
      "photoUrl": "",
      "userId": "06209305298175359431"
     },
     "user_tz": 300
    },
    "id": "OrE8K9bNBpd6",
    "outputId": "285cbcbe-e971-4feb-df06-575ed52db69b"
   },
   "outputs": [],
   "source": [
    "# focus_dic = {'1':{'seg_threshold':45,\n",
    "#                  'seg_point_distance':35,\n",
    "#                  'mesh_spaces': 7,\n",
    "#                  'min_order' : 1}, \n",
    "#               '2':{'seg_threshold':18,\n",
    "#                    'seg_point_distance':2,\n",
    "#                    'mesh_spaces': 5,\n",
    "#                    'min_dem2river_distance' : 8,\n",
    "#                    'min_order' : 1},\n",
    "#                '3':{'seg_threshold':20,\n",
    "#                    'seg_point_distance':5,\n",
    "#                    'mesh_spaces': 6,\n",
    "#                    'min_dem2river_distance' : 15,\n",
    "#                    'min_order' : 1}\n",
    "# }\n",
    "\n",
    "\n",
    "focus_dic = {'1':{'seg_threshold':55,\n",
    "                 'seg_point_distance':7,\n",
    "                 'mesh_spaces': 4,\n",
    "                 'min_order' : 1}, \n",
    "              '2':{'seg_threshold':50,\n",
    "                   'seg_point_distance':5,\n",
    "                   'mesh_spaces': 3,\n",
    "                   'min_dem2river_distance' : 7,\n",
    "                   'min_order' : 1},\n",
    "               '3':{'seg_threshold':45,\n",
    "                   'seg_point_distance':4,\n",
    "                   'mesh_spaces': 2,\n",
    "                   'min_dem2river_distance' : 6,\n",
    "                   'min_order' : 1}\n",
    "}\n",
    "\n",
    "\n",
    "gp = go.ghost_preprocess(cu, path_dem='dem.tif', \n",
    "    #seg_threshold=800,\n",
    "  #seg_point_distance=75,\n",
    "  focus_map = 'focus_dic.TIF',\n",
    "  focus_dict = focus_dic\n",
    "  )\n",
    "\n",
    "corrected = gp.get_segments_topology(epsilon=0.1)\n",
    "gp.select_segments_using_focus(1)\n",
    "gp.get_mesh_river_points(clean_close_points=True,\n",
    "                         min_river2river_distance=50)\n",
    "gp.get_mesh_grid_points(mesh_spaces=12,\n",
    "                        border_iter=2,\n",
    "                        clean_with_river=True,\n",
    "                        min_dem2river_distance=50)\n",
    "\n",
    "gp.get_voronoi_polygons()\n",
    "gp.define_polygons_topology()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "2juqz_Zm94KF"
   },
   "outputs": [],
   "source": [
    "gp.write_mesh_file('meshFocus.mesh','meshFocus.shp')\n",
    "gp.write_river_file(path = 'examplFocus.riv', \n",
    "                    shp_path = 'example_rivFocus.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnc1r6_3dppk"
   },
   "source": [
    "# Use earth explorer functions to set up the soil properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 453796,
     "status": "ok",
     "timestamp": 1629391555115,
     "user": {
      "displayName": "Sam Landsteiner",
      "photoUrl": "",
      "userId": "06209305298175359431"
     },
     "user_tz": 300
    },
    "id": "HiJQwYdt9_VG",
    "outputId": "dad557e6-d49e-47e8-f5ca-6645f65bd83d"
   },
   "source": [
    "#Gets the soil data\n",
    "soils = go.get_soils_data()\n",
    "gp.get_physical_prop(soils, sliced=True, xdivisions=5, ydivisions=5, prop_name='soils')\n",
    "#Fill the geometries with null values.\n",
    "gp.polygons_shp['soils'].fillna(gp.polygons_shp['soils'].value_counts().idxmax(), \n",
    "                                inplace = True)\n",
    "#Make the variable an integer\n",
    "gp.polygons_shp['soils'] = gp.polygons_shp['soils'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 397761,
     "status": "ok",
     "timestamp": 1629391952873,
     "user": {
      "displayName": "Sam Landsteiner",
      "photoUrl": "",
      "userId": "06209305298175359431"
     },
     "user_tz": 300
    },
    "id": "dPErzYCp_9fL",
    "outputId": "9a22fe40-8c2a-48cd-eec8-c62ff900296c"
   },
   "source": [
    "#Gets the land use data\n",
    "land_use = go.get_land_use_data()\n",
    "gp.get_physical_prop(land_use, sliced=True, xdivisions=5, ydivisions=5, prop_name='land', band = 'cropland')\n",
    "#Make the vartiable and integer\n",
    "gp.polygons_shp['land'] = gp.polygons_shp['land'].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgGlyZHrPyqK"
   },
   "source": [
    "gp.write_attribute_file('/content/drive/MyDrive/GHOST/Outputs/example.att', \n",
    "                        'soils', \n",
    "                        'land')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attribute file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the shapefile\n",
    "shapefile_path = 'meshFocus.shp'\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "# Save the attribute table to a CSV file\n",
    "output_csv_path = 'meshFocus.csv'\n",
    "gdf.drop(columns='geometry').to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the newly created shapefile CSV and the BearCreek CSV\n",
    "shapefile_csv = pd.read_csv(output_csv_path)\n",
    "csv_path = 'BearCreek-att.csv'\n",
    "csv_df = pd.read_csv(csv_path)\n",
    "\n",
    "# Ensure the 'polygon' and 'nfaces' columns exist in the shapefile CSV\n",
    "if 'polygon' not in shapefile_csv.columns or 'nfaces' not in shapefile_csv.columns:\n",
    "    raise ValueError(\"The 'polygon' and/or 'nfaces' columns are missing in the shapefile CSV.\")\n",
    "\n",
    "# Extend the BearCreek-att CSV to have the same number of rows as the shapefile CSV if needed\n",
    "if len(csv_df) < len(shapefile_csv):\n",
    "    rows_to_add = len(shapefile_csv) - len(csv_df)\n",
    "    extra_rows = pd.DataFrame(index=range(rows_to_add), columns=csv_df.columns)\n",
    "    csv_df = pd.concat([csv_df, extra_rows], ignore_index=True)\n",
    "\n",
    "# Copy the 'polygon' and 'nfaces' values to the 'INDEX' and 'nfaces' columns in the BearCreek CSV\n",
    "csv_df['INDEX'] = shapefile_csv['polygon']\n",
    "csv_df['nfaces'] = shapefile_csv['nfaces']\n",
    "\n",
    "# Fields to extend with the same values till the end of the 'INDEX' column\n",
    "fields_to_extend = ['SOIL', 'LC', 'METEO', 'LAI', 'SS', 'LAKE', 'CLOSE_SEG']\n",
    "for field in fields_to_extend:\n",
    "    if field in csv_df.columns:\n",
    "        last_value = csv_df[field].iloc[0]\n",
    "        csv_df[field].fillna(last_value, inplace=True)\n",
    "    else:\n",
    "        csv_df[field] = csv_df[field].iloc[0]\n",
    "\n",
    "# Save the updated BearCreek-att CSV\n",
    "output_csv_path_updated = 'Upper-att.csv'\n",
    "csv_df.to_csv(output_csv_path_updated, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Read data from CSV file\n",
    "df = pd.read_csv('Upper-att.csv')\n",
    "\n",
    "# Iterate through rows and add zeros to BCns\n",
    "for index, row in df.iterrows():\n",
    "    nfaces = int(row['nfaces'])  # Cast to integer\n",
    "    zeros_to_add = nfaces - 1  # Subtract 1 to account for the existing zero in the BCns column\n",
    "    zeros = [0] * zeros_to_add\n",
    "    new_bcns = [0] + zeros  # Add the initial zerat o and then the additional zeros\n",
    "    df.at[index, 'BCns'] = ','.join(map(str, new_bcns))\n",
    "\n",
    "# Replace commas with tabs and remove quotes in the BCns column\n",
    "df['BCns'] = df['BCns'].apply(lambda x: str(x).replace(',', '\\t').replace('\"', ''))\n",
    "\n",
    "# Replace commas with tabs for all fields\n",
    "df_with_tabs = df.applymap(lambda x: str(x).replace(',', '\\t'))\n",
    "\n",
    "# Write the modified data back to a CSV file\n",
    "df_with_tabs.to_csv('Upper_Creek.att', index=False, sep='\\t', quoting=csv.QUOTE_NONE, escapechar=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 - 100 m\n",
    "import pandas as pd\n",
    "\n",
    "# Read data from CSV file\n",
    "# df = pd.read_csv('BearCreek-att.csv')\n",
    "df = pd.read_csv('BearCreek-att.csv')\n",
    "# Iterate through rows and add zeros to BCns\n",
    "for index, row in df.iterrows():\n",
    "    nfaces = int(row['nfaces'])  # Cast to integer\n",
    "    zeros_to_add = nfaces - 1  # Subtract 1 to account for the existing zero in the BCns column\n",
    "    zeros = [0] * zeros_to_add\n",
    "    new_bcns = [0] + zeros  # Add the initial zero and then the additional zeros\n",
    "    df.at[index, 'BCns'] = ','.join(map(str, new_bcns))\n",
    "\n",
    "# Write the modified data back to a CSV file\n",
    "df.to_csv('BearCreek-Mesh_1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBWiNJTNiQQ3"
   },
   "source": [
    "# Append attribute file to account for forcing/tiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ul5pPFLQuf1L"
   },
   "source": [
    "## Read in meteo pixels as gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZjRlHUciZ9V"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as geop  # Import geopandas as gpd\n",
    "\n",
    "# read in pixels.csv from meteo gen file\n",
    "df = pd.read_csv('pixels.csv')\n",
    "\n",
    "# convert df to geodataframe point class\n",
    "pixel_gdf = geop.GeoDataFrame(\n",
    "    df, geometry=geop.points_from_xy(df.lon, df.lat), crs=4326)\n",
    "\n",
    "# convert gdf crs to match project (lat to utm)\n",
    "pixel_gdf = pixel_gdf.to_crs(26915)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wr22_bcnusjP"
   },
   "source": [
    "## Read in mesh as gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s2E6PcRpq3fW"
   },
   "outputs": [],
   "source": [
    "# read in numele from mesh.mesh\n",
    "with open('mesh.mesh') as f:\n",
    "  first_line = f.readline()\n",
    "  f.close()\n",
    "\n",
    "# obtain value for numele from string\n",
    "npoly = [int(i) for i in first_line.split() if i.isdigit()]\n",
    "\n",
    "# create df that excludes geometry data appendage\n",
    "df = pd.read_csv('mesh.mesh', names=['Index','X','Y','Zmin','Zmax','Area','nFaces'], delimiter=\"\\t\", header=1, nrows=npoly[0] )\n",
    "\n",
    "# create mesh_gdf\n",
    "mesh_gdf = geop.GeoDataFrame(\n",
    "    df, geometry=geop.points_from_xy(df.X, df.Y), crs=26915)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McNIi9_j6axb"
   },
   "source": [
    "## Read in river file as gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sC9J6ZOA6gF-"
   },
   "outputs": [],
   "source": [
    "# read in numele from exampl.riv\n",
    "with open('exampl.riv') as f:\n",
    "  first_line = f.readline()\n",
    "  f.close()\n",
    "\n",
    "# obtain value for numele from string\n",
    "nriv = [int(i) for i in first_line.split() if i.isdigit()]\n",
    "\n",
    "# create df that excludes geometry data appendage\n",
    "names = ['Index',\t'X',\t'Y',\t'ZMIN',\t'ZMAX',\t'LENGTH',\t'DOWN',\t'LEFT',\t'RIGHT',\t'SHAPE',\t'MATRL',\t'BC',\t'RES',\t'XAREA',\t'INACT',\t'LAKE',\t'LRIV']\n",
    "df = pd.read_csv('exampl.riv', names=names, delimiter=\"\\t\", header=1, nrows=nriv[0] )\n",
    "\n",
    "# create riv_gdf\n",
    "riv_gdf = geop.GeoDataFrame(\n",
    "    df, geometry=geop.points_from_xy(df.X, df.Y), crs=26915)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCqxagiS-xYm"
   },
   "source": [
    "## Read in attribute file as df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "98chduQM0aLg"
   },
   "outputs": [],
   "source": [
    "# define nico's function for reading in attribute file\n",
    "def read_attfile(path):\n",
    "  f = open(path,'r')\n",
    "  lines = f.readlines()\n",
    "  f.close()\n",
    "  z = [i.split()[:8] for i in lines[1:]]\n",
    "  df = pd.DataFrame(z, columns=lines[0].split(' ')[:8])\n",
    "  df = df.astype(int)\n",
    "  return df\n",
    "\n",
    "att_df = read_attfile('example.att')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0m_1v-UC2gr"
   },
   "source": [
    "## Correct meteo assignment for each polygon in attribute file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "executionInfo": {
     "elapsed": 227141,
     "status": "ok",
     "timestamp": 1629392181570,
     "user": {
      "displayName": "Sam Landsteiner",
      "photoUrl": "",
      "userId": "06209305298175359431"
     },
     "user_tz": 300
    },
    "id": "izhjKDeqDBMH",
    "outputId": "3ab8d1d1-7029-4aa1-ada4-b1fc830d913a"
   },
   "outputs": [],
   "source": [
    "def get_nearest_values(row, other_gdf, point_column='geometry', value_column=\"geometry\"):\n",
    "    \"\"\"Find the nearest point and return the corresponding value from specified value column.\"\"\"\n",
    "\n",
    "    # Create an union of the other GeoDataFrame's geometries:\n",
    "    other_points = other_gdf[\"geometry\"].unary_union\n",
    "\n",
    "    # Find the nearest points\n",
    "    nearest_geoms = shply.ops.nearest_points(row[point_column], other_points)\n",
    "\n",
    "    # Get corresponding values from the other df\n",
    "    nearest_data = other_gdf.loc[other_gdf[\"geometry\"] == nearest_geoms[1]]\n",
    "    nearest_value = nearest_data[value_column].to_numpy()[0]\n",
    "\n",
    "    return nearest_value\n",
    "\n",
    "# append mesh_gdf to include meteo value for each polygon\n",
    "mesh_gdf[\"nearest_idx\"] = mesh_gdf.apply(get_nearest_values, other_gdf=pixel_gdf, value_column=\"ID\", axis=1)\n",
    "\n",
    "# check min/max pixel id used\n",
    "max_idx = max(mesh_gdf[\"nearest_idx\"])\n",
    "display(max_idx)\n",
    "min_idx = min(mesh_gdf[\"nearest_idx\"])\n",
    "display(min_idx)\n",
    "\n",
    "#replace the meteo value in attr with pixel value from mesh\n",
    "att_df[\"METEO\"] = mesh_gdf[\"nearest_idx\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vl44I2tx-QeE"
   },
   "source": [
    "## Correct close_seg assignment for each polygon in attribute file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "executionInfo": {
     "elapsed": 1896986,
     "status": "ok",
     "timestamp": 1629394078555,
     "user": {
      "displayName": "Sam Landsteiner",
      "photoUrl": "",
      "userId": "06209305298175359431"
     },
     "user_tz": 300
    },
    "id": "lGuwBrlWxz68",
    "outputId": "8887d18e-9d8f-42c3-e695-5e824be50938"
   },
   "outputs": [],
   "source": [
    "# append mesh_gdf to include close_seg value for each polygon\n",
    "mesh_gdf[\"close_seg\"] = mesh_gdf.apply(get_nearest_values, other_gdf=riv_gdf, value_column=\"Index\", axis=1)\n",
    "\n",
    "# check min/max close_seg used\n",
    "max_seg = max(mesh_gdf[\"close_seg\"])\n",
    "display(max_seg)\n",
    "min_seg = min(mesh_gdf[\"close_seg\"])\n",
    "display(min_seg)\n",
    "\n",
    "#replace the meteo value in attr with pixel value from mesh\n",
    "att_df[\"CLOSE_SEG\"] = mesh_gdf[\"close_seg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U1VWDI_Thtlk"
   },
   "outputs": [],
   "source": [
    "#replace the meteo value in attr with pixel value from mesh\n",
    "att_df[\"CLOSE_SEG\"] = mesh_gdf[\"close_seg\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uhvck7qjMb6w"
   },
   "source": [
    "## Write updated attribute file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1d-xnz1okvFa"
   },
   "outputs": [],
   "source": [
    "npoly=npoly[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eG0zxjR9MEub"
   },
   "outputs": [],
   "source": [
    "def write_att(mesh, old_att, path, npoly):\n",
    "    f = open(path, 'w')\n",
    "    f.write('INDEX SOIL LC METEO LAI SS LAKE CLOSE_SEG BCns\\n')\n",
    "    for i in range(npoly):\n",
    "        poly = mesh.loc[i,'Index']\n",
    "        nfaces = mesh.loc[i,'nFaces']\n",
    "        #soil = old_att.loc[i,'SOIL']\n",
    "        #land = old_att.loc[i,'LC']\n",
    "        meteo = old_att.loc[i,'METEO']\n",
    "        close_seg = old_att.loc[i,'CLOSE_SEG']\n",
    "        #f.write('%d %d %d %d 1 0 0 %d ' % (poly, soil, land, meteo, close_seg))\n",
    "        f.write('%d 1 12 %d 1 0 0 %d ' % (poly, meteo, close_seg))\n",
    "        for z in range(nfaces):\n",
    "            f.write('0 ')\n",
    "        f.write('\\n')\n",
    "    f.close()\n",
    "\n",
    "write_att(mesh=mesh_gdf, old_att=att_df, path='/content/drive/MyDrive/GHOST/Outputs/revised.att', npoly=npoly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sYDds4KwgbQw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "flhSIabNpyQ2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "executionInfo": {
     "elapsed": 275,
     "status": "ok",
     "timestamp": 1629919324314,
     "user": {
      "displayName": "Nicolás Velásquez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj_p27Ww8CzJCwMccm7KvzOMrLqhxfXelzHuWR1GA=s64",
      "userId": "15649486147728645342"
     },
     "user_tz": 300
    },
    "id": "T-1x2gs6wvgd",
    "outputId": "4a322102-ee7d-4475-c564-941dd173fa0e"
   },
   "outputs": [],
   "source": [
    "cu.Plot_basinClean(cu.structure[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oYo5-YFEwyXk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
